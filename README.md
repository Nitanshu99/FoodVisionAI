# FoodVisionAI ü•ò

### High-Fidelity Monocular Dietary Assessment System

**FoodVisionAI** is a research-grade, offline computer vision system designed to estimate the nutritional content of food from single images. It moves beyond simple classification by integrating **Geometric Deep Learning** (Volume Estimation) with a **Dual-Model "Split-Brain" Architecture** (Context vs. Texture Analysis) to handle complex dietary scenarios like Indian Thalis.

---

## üöÄ Key Features

* **üß† "Split-Brain" Inference Engine:** Automatically switches strategies based on scene complexity:
  * **Global Context Model:** Used for single dishes to capture plate context and scale.
  * **Local Specialist Model:** Used for multi-item meals (Thalis) to classify specific food textures from tight crops.
* **üí¨ Conversational UI:** A ChatGPT-style interface that allows users to upload meals, receive visual/nutritional reports, and (future) ask follow-up questions.
* **üìä Physics-Based Nutrient Engine:** Calculates mass using volumetric primitives (Cylinders, Spherical Caps) and density lookups, rather than simple regression.
* **üîó Rich Metadata Integration:** Fetches Ingredients, Standard Serving Sizes, and Recipe Source URLs from a local Parquet database.
* **üìù RAG-Ready Logging:** Automatically saves structured JSON logs (`food_item_1`, `food_item_2`...) for future LLM integration.

---

## üèóÔ∏è System Architecture

The system operates on a **Hybrid Pipeline**:

### 1. Vision Layer (The Eyes)

* **YOLOv8-Seg:** Detects food items, generates segmentation masks, and estimates Real-World Scale (Pixels-Per-Metric) using a reference plate diameter (28cm).
* **Decision Logic:**
  * If N ‚â§ 1 Objects: **Global Path** (Uses Full Image).
  * If N > 1 Objects: **Local Path** (Uses Cropped Objects).

### 2. Classification Layer (The Brain)

* **Global Model (`model_best.keras`):** EfficientNet-B5 trained on standardized, full-context images.
* **Local Model (`model_yolo_best.keras`):** EfficientNet-B5 trained on "Clean" YOLO-generated crops (Background removed).

### 3. Nutrient Layer (The Logic)

* Combines **Volume (cm¬≥)** √ó **Density (g/cm¬≥)** to get Mass.
* Queries **INDB (Indian Food Database)** for macros.
* Enriches data with Recipe Links and Ingredients.

---

## üìÇ Directory Structure

```
FoodVisionAI/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Original raw image datasets
‚îÇ   ‚îú‚îÄ‚îÄ processed/            # Global Model Data (Standardized & Split)
‚îÇ   ‚îú‚îÄ‚îÄ yolo_processed/       # Local Model Data (YOLO Cropped & Split)
‚îÇ   ‚îú‚îÄ‚îÄ parquet_db/           # Database Engine (INDB, Recipes, Units, Links)
‚îÇ   ‚îî‚îÄ‚îÄ inference_logs/       # JSON logs stored here for RAG
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_best.keras       # Global Model (Context-Aware)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_yolo_best.keras  # Local Model (Crop-Specialist)
‚îÇ   ‚îî‚îÄ‚îÄ yolov8m-seg.pt        # Segmentation Model
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_tools/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ folder_mapper.py       # Preprocessor A: Maps/Prunes raw data -> data/processed
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ yolo_processor.py      # Preprocessor B: Crops objects -> data/yolo_processed
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inspect_headers.py     # Database diagnostic tool
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ augmentation.py       # Custom Augmentation (RandomGaussianBlur)
‚îÇ   ‚îú‚îÄ‚îÄ config.py             # Central Configuration
‚îÇ   ‚îú‚îÄ‚îÄ nutrient_engine.py    # Physics & Database Logic
‚îÇ   ‚îú‚îÄ‚îÄ segmentation.py       # YOLOv8 Geometry Engine
‚îÇ   ‚îú‚îÄ‚îÄ vision_model.py       # EfficientNet-B5 Architecture
‚îÇ   ‚îî‚îÄ‚îÄ vision_utils.py       # "Split-Brain" Inference Logic
‚îÇ
‚îú‚îÄ‚îÄ app.py                    # Streamlit Frontend (Chat Interface)
‚îú‚îÄ‚îÄ train.py                  # Training script (Global Model)
‚îú‚îÄ‚îÄ after_yolo_train.py       # Training script (Local Model)
‚îî‚îÄ‚îÄ requirements.txt          # Project Dependencies
```

---

## üîÑ Data Workflow

The system employs two distinct data preprocessing pipelines to create specialized datasets for the "Split-Brain" architecture.

### 1. The Database Pipeline

Raw Excel/CSV files are converted into high-speed **Parquet** files for sub-millisecond querying:

* `INDB.parquet`: Macronutrients (Energy, Protein, Carbs, Fat).
* `recipes.parquet`: Ingredient lists.
* `recipe_links.parquet`: Source URLs.
* `Units.parquet`: Density estimation (Cup/Bowl weights).

### 2. The Training Pipeline (Dual-Stream)

#### Stream A: The Global Model Pipeline (Context-Aware)

* **Input:** `data/raw/images`
* **Preprocessing:** Run `src/data_tools/folder_mapper.py`.
  * **Normalization:** Standardizes folder names (e.g., "chicken pizza" ‚Üí `BFP122`).
  * **Pruning:** Removes irrelevant or unmapped folders.
  * **Splitting:** Generates Train/Val splits in `data/processed`.
* **Training:**
  ```bash
  python train.py
  ```

#### Stream B: The Local Model Pipeline (Texture-Specialist)

* **Input:** `data/raw/images`
* **Preprocessing:** Run `src/data_tools/yolo_processor.py`.
  * **Detection:** Uses YOLOv8 to find the specific food object.
  * **Cropping:** Removes background/plate context to isolate texture.
  * **Splitting:** Generates Train/Val splits in `data/yolo_processed`.
* **Training:**
  ```bash
  python after_yolo_train.py
  ```

---

## üë§ User Guide

### Installation

```bash
# Clone and Enter
git clone https://github.com/yourusername/FoodVisionAI.git
cd FoodVisionAI

# Install Dependencies
pip install -r requirements.txt
```

### Running the App

Launch the Chat Interface:

```bash
streamlit run app.py
```

### Using the System

1. **Upload:** Drop a meal photo in the sidebar.
2. **Analysis:**
   * **Single Dish:** The AI detects the context and identifies the dish (e.g., *Uttapam*) using the **Global Model**.
   * **Thali:** The AI detects multiple items (e.g., *Roti, Paneer, Dal*), crops them, and identifies each individually using the **Local Model**.
3. **Result:**
   * View the **annotated image** (Green boxes).
   * Read the **Nutritional Summary** card.
   * Click **"Recipe Link"** to see how to make it.
4. **Logging:**
   * Check `data/inference_logs/` to see the generated JSON file with detailed breakdown (`food_item_1`, `food_item_2`...) ready for LLM usage.

---

## üß© JSON Log Format (RAG-Ready)

Every prediction is saved automatically in this structure:

```json
{
    "timestamp": "2023-10-27T10:00:00",
    "total_summary": {
        "Energy (kcal)": 650,
        "Protein (g)": 25
    },
    "food_item_1": {
        "name": "Paneer Butter Masala",
        "mass_g": 150.0,
        "macros": { "calories": 450, "protein": 18 },
        "metadata": {
            "ingredients": ["Paneer", "Tomato", "Cashew"],
            "source": "https://hebbarskitchen.com/...",
            "model_used": "Object Crop (Local Specialist)"
        }
    },
    "food_item_2": {
        "name": "Tandoori Roti",
        "mass_g": 60.0,
        ...
    }
}
```

---

## üî¨ Technical Specifications

### Model Architecture

Both Global and Local models share the same core architecture, optimized for the NVIDIA RTX A6000:

* **Backbone:** EfficientNet-B5 (Pretrained on ImageNet)
* **Input Resolution:** 512 √ó 512 pixels
  * **Why:** High resolution is required to detect fine-grained textures (e.g., oil separation, spice patterns)
* **Activation Function:** Swish (SiLU)
  * **Why:** Unlike ReLU (used in older ResNets), Swish is a smooth, non-monotonic function (f(x) = x ¬∑ sigmoid(x)) that allows a small amount of negative information to flow through. This is mathematically proven to help deep networks like B5 learn complex patterns (like food textures) faster.
* **Training Hardware:** NVIDIA RTX A6000 (48GB VRAM)
* **Model Parameters:** ~30M parameters per model

#### Hyperparameters

* **Batch Size:** 64 (Optimal balance of speed and Batch Norm stability)
* **Optimizer:** AdamW (Weight Decay: 1e-4)
* **Learning Rate:** 1e-3 with Cosine Decay Schedule
* **Precision:** FP32 (Standard 32-bit Floating Point)

### Data Augmentation Strategy (The "Regularization Shield")

**Theory:** EfficientNet-B5 is a high-capacity model (30M+ parameters). Given the "small local-food dataset," the risk of overfitting (memorizing noise instead of features) is mathematically high. We introduce a stochastic "Variance Injection" pipeline to enforce Rotational, Scale, and Photometric Invariance, ensuring the model generalizes to unseen real-world photos.

#### Pipeline Specifications (Applied Pre-Batch)

**Geometric Transformations (Spatial Invariance):**

* **Random Rotation:** ¬±20¬∞
  * **Justification:** User photos will rarely be perfectly aligned.
* **Random Zoom:** Range [0.8, 1.2] (20% zoom in/out)
  * **Justification:** Addresses the "Distance Ambiguity" problem where the food's scale varies relative to the camera frame.
* **Random Flip:** Horizontal
  * **Justification:** Food presentation is chirality-invariant (a samosa facing left is still a samosa).

**Photometric Transformations (Sensor Invariance):**

* **Random Contrast & Brightness:** Factor 0.2
  * **Justification:** Simulates varying lighting conditions (e.g., dim restaurant vs. bright kitchen).

**Blur Transformations (Focus Invariance):**

* **Gaussian Blur:** Kernel Size 3√ó3 or 5√ó5, applied with 30% probability
  * **Why:** This simulates Depth of Field and Motion Blur (common in hasty smartphone photography). It forces the model to learn "shape" and "global structure" rather than relying solely on sharp, high-frequency textures that might disappear in a blurry photo.

### Split-Brain Architecture

The dual-model approach solves a fundamental challenge in food recognition:

* **Global Model** excels at understanding spatial relationships and portion context
* **Local Model** specializes in identifying foods when isolated from their surroundings

This architecture automatically adapts based on scene complexity, providing optimal accuracy for both simple plates and complex multi-dish meals.

### Physics-Based Volume Estimation

Unlike traditional regression approaches, FoodVisionAI uses geometric primitives to calculate actual food volume:

* Cylinders for flat/round foods (rotis, dosas)
* Spherical caps for mounded foods (rice, curries)
* Real-world scale calibration using standard plate dimensions (28cm reference)

### RAG Integration Ready

The structured JSON logging format enables seamless integration with Large Language Models for:

* Conversational dietary advice
* Meal planning recommendations
* Nutritional queries and explanations

### Technical Justifications

**Why Parquet?**

"We converted legacy Excel datasets to Apache Parquet. This provides schema enforcement (preventing data type errors) and allows our Logic Engine to query nutritional priors in milliseconds, ensuring a seamless user experience."

**Why Swish Activation?**

"We utilized the Swish activation function (standard in EfficientNet) because its smooth, non-linear property prevents 'dying neurons' during the training of deep networks, allowing our B5 model to capture subtle food texture gradients better than ReLU."

**Why Batch Size 64?**

"Leveraging the 48GB VRAM of the A6000, we maximized the batch size to 64. This ensures stable statistical estimation in the Batch Normalization layers, which is critical for generalizing across the high intra-class variance of Indian food."

---

## üìä Performance Characteristics

* **Inference Speed:** ~2-3 seconds per image (CPU), ~0.5 seconds (GPU)
* **Model Size:** 
  * Global Model: ~115MB
  * Local Model: ~115MB
  * YOLO Segmentation: ~50MB
* **Database Query:** Sub-millisecond (Parquet optimization)
* **Supported Foods:** Indian cuisine focus with extensible architecture

---

## üõ†Ô∏è Development Workflow

### Training the Global Model

```bash
# 1. Prepare raw data
# Place images in data/raw/images/

# 2. Preprocess and map to food codes
python src/data_tools/folder_mapper.py

# 3. Train the global model
python train.py
```

### Training the Local Model

```bash
# 1. Generate YOLO crops
python src/data_tools/yolo_processor.py

# 2. Train the local specialist
python after_yolo_train.py
```

### Database Setup

All database files should be placed in `data/raw/metadata/`:
* `INDB.xlsx`
* `recipes.xlsx`
* `recipes_names.xlsx`
* `recipes_servingsize.xlsx`
* `recipe_links.xlsx`
* `Units.xlsx`

These will be automatically converted to Parquet format during preprocessing.

---

## ü§ù Contributing

Contributions are welcome! Areas of interest:

* Expanding food database coverage
* Improving volume estimation algorithms
* Adding support for additional cuisines
* Enhancing the conversational UI

---

## üìÑ License

[Your License Here]

---

## üôè Acknowledgments

* Indian Food Database (INDB) for nutritional data
* YOLOv8 for state-of-the-art object detection
* EfficientNet for efficient image classification
* Hebbar's Kitchen and other recipe sources

---

## üìß Contact

[Your Contact Information]