"""
Unified Training Script (With Class Filtering).

Trains the final classifier using the 'Clean Object' dataset
generated by yolo_processor.py.

Auto-optimized for hardware using auto_config.py
"""

import os
import numpy as np
import tensorflow as tf
import keras
from keras import callbacks
from pathlib import Path

# Local imports
import config
from src.models.builder import build_model
from src.models.augmentation import get_augmentation_pipeline, RandomGaussianBlur
from src.models.loader import load_model as load_keras_model

# Auto-configuration for hardware optimization
try:
    from config.hardware import get_auto_config
    AUTO_CONFIG = get_auto_config()
    BATCH_SIZE = AUTO_CONFIG.get('TRAIN_BATCH_SIZE', config.BATCH_SIZE)
    print(f"\n‚úÖ Using auto-optimized settings:")
    print(f"   Batch Size: {BATCH_SIZE}")
    print(f"   GPU: {AUTO_CONFIG.get('GPU_NAME', 'N/A')}")
    print(f"   Precision: {AUTO_CONFIG.get('TRAIN_PRECISION', 'fp32')}")
except (ImportError, KeyError):
    print("\n‚ö†Ô∏è  Auto-config not found, using defaults")
    AUTO_CONFIG = None
    BATCH_SIZE = config.BATCH_SIZE

# --- CONFIGURATION ---
BASE_DIR = Path(__file__).resolve().parent
TRAIN_DIR = config.DATA_DIR / "yolo_processed" / "train"
VAL_DIR = config.DATA_DIR / "yolo_processed" / "val"
CLASS_NAMES_PATH = config.LABELS_PATH
FINAL_MODEL_PATH = config.MODEL_LOCAL_PATH  # Saving to Local Model path

# --- üö´ BLACKLIST: Add folders here to skip them ---
EXCLUDED_CLASSES = ["ASC321"]

# Performance Tuning
AUTOTUNE = tf.data.AUTOTUNE
IMG_SIZE = config.IMG_SIZE

def load_dataset(directory: Path, allowed_classes: list, is_training: bool = False):
    """Loads dataset with class balancing and returns (dataset, class_names)."""
    if not directory.exists():
        raise FileNotFoundError(f"‚ùå Dataset not found at {directory}.")

    print(f"Loading data from: {directory}...")

    # Count images per class
    class_counts = {}
    for cls in allowed_classes:
        cls_path = directory / cls
        if cls_path.exists():
            class_counts[cls] = len(list(cls_path.glob("*")))

    # Calculate samples per class based on original count
    samples_per_class = {}
    for cls, count in class_counts.items():
        if count >= 5000:
            samples_per_class[cls] = 20000  # Cap at 20k
        elif count >= 500:
            samples_per_class[cls] = 20000  # Cap at 20k
        elif count >= 100:
            samples_per_class[cls] = 10000  # Cap at 10k
        else:
            samples_per_class[cls] = 2500   # Cap at 2.5k

    print(f"\nüìä Class Balancing Strategy:")
    for cls, target in sorted(samples_per_class.items()):
        orig = class_counts[cls]
        print(f"   {cls}: {orig} ‚Üí {target} samples (√ó{target/orig:.1f})")

    # Load Image Dataset with explicit class filtering
    ds = keras.utils.image_dataset_from_directory(
        directory,
        labels="inferred",
        label_mode="categorical",
        class_names=allowed_classes,
        color_mode="rgb",
        batch_size=1,  # Small batch for resampling
        image_size=IMG_SIZE,
        shuffle=False,
        seed=42
    )

    class_names = list(ds.class_names) if hasattr(ds, 'class_names') else allowed_classes
    ds = ds.unbatch()  # type: ignore  # Unbatch for resampling

    # Apply class balancing through resampling
    if is_training:
        # Create class-wise datasets
        class_datasets = []
        for class_idx, cls in enumerate(class_names):
            # Filter for this class
            class_ds = ds.filter(lambda x, y: tf.argmax(y) == class_idx)

            # Repeat and take target samples
            target_samples = samples_per_class[cls]
            class_ds = class_ds.repeat().take(target_samples)

            class_datasets.append(class_ds)

        # Interleave all classes
        ds = tf.data.Dataset.sample_from_datasets(
            class_datasets,
            weights=[1.0] * len(class_datasets),
            seed=42
        )

        # Apply augmentation
        aug_pipeline = get_augmentation_pipeline()
        ds = ds.map(
            lambda x, y: (aug_pipeline(x, training=True), y),
            num_parallel_calls=AUTOTUNE
        )

    # Batch and prefetch
    ds = ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)

    return ds, class_names

def get_filtered_class_list(directory: Path):
    """Scans directory and removes blacklisted classes."""
    # Get all folder names
    all_classes = sorted([d.name for d in directory.iterdir() if d.is_dir()])
    
    # Filter
    valid_classes = [c for c in all_classes if c not in EXCLUDED_CLASSES]
    
    print(f"\nüîç Class Filtering Report:")
    print(f"   - Total Folders Found: {len(all_classes)}")
    print(f"   - Excluded: {EXCLUDED_CLASSES}")
    print(f"   - Final Class Count: {len(valid_classes)}")
    
    return valid_classes

def main():
    print(f"\nüöÄ Starting Unified Model Training on Device: {config.DEVICE}")
    print(f"TensorFlow Version: {tf.__version__}")
    print(f"üìÇ Training Data: {TRAIN_DIR}")

    # Enable TF32 for A6000 (2-3x faster matrix operations)
    if AUTO_CONFIG and AUTO_CONFIG.get('GPU_NAME', '').startswith('NVIDIA RTX A6000'):
        print(f"\n‚ö° Enabling TensorFloat-32 (TF32) for A6000...")
        tf.config.experimental.enable_tensor_float_32_execution(True)
        print(f"   TF32 enabled: 2-3x faster matrix operations!")

    # Enable XLA JIT compilation for faster execution
    print(f"‚ö° Enabling XLA JIT compilation...")
    tf.config.optimizer.set_jit(True)
    print(f"   XLA JIT enabled: 1.5-2x faster execution!")

    # Enable mixed precision for A6000 (FP16)
    # Note: TensorFlow requires special handling for FP16 with categorical outputs
    # Using float32 for now to avoid compatibility issues
    if AUTO_CONFIG and AUTO_CONFIG['TRAIN_PRECISION'] == 'fp16':
        print(f"\n‚ö° Mixed Precision available but disabled for TensorFlow compatibility")
        print(f"   (TensorFlow categorical outputs require float32)")
        # tf.keras.mixed_precision.set_global_policy('mixed_float16')  # Disabled

    # 1. Get List of Allowed Classes
    target_classes = get_filtered_class_list(TRAIN_DIR)

    if not target_classes:
        print("‚ùå Error: No valid classes found after filtering!")
        print("üí° Hint: Run 'python src/data_tools/yolo_processor.py' first to generate training data.")
        return

    # 2. Load Data (Now unpacking the tuple)
    train_ds, class_names = load_dataset(TRAIN_DIR, target_classes, is_training=True)
    val_ds, _ = load_dataset(VAL_DIR, target_classes, is_training=False)
    
    # 3. Save Class Names
    print(f"‚úÖ Detected {len(class_names)} Classes.")
    np.save(CLASS_NAMES_PATH, class_names)
    print(f"üíæ Class names saved to {CLASS_NAMES_PATH}")

    # 4. Build or Resume Model
    initial_epoch = 0
    
    if os.path.exists(FINAL_MODEL_PATH):
        print(f"\n--- Resuming from Checkpoint: {FINAL_MODEL_PATH} ---")
        try:
            # Load model with custom objects for augmentation layers
            model = load_keras_model(FINAL_MODEL_PATH, compile=True)
            print(">> Model and training state loaded successfully.")

            # TODO: Extract epoch number from model if needed for initial_epoch
            # For now, ModelCheckpoint will handle best model restoration

        except Exception as e:
            print(f">> Warning: Could not load checkpoint ({e}). Building new model.")
            model = build_model(num_classes=len(class_names))
    else:
        print("\nüèóÔ∏è Building New EfficientNet-B5 Model...")
        model = build_model(num_classes=len(class_names))
    
    # 5. Callbacks
    callbacks_list = [
        callbacks.ModelCheckpoint(
            filepath=str(FINAL_MODEL_PATH),
            save_best_only=True,
            monitor="val_accuracy",
            mode="max",
            verbose=1
        ),
        callbacks.EarlyStopping(
            monitor="val_accuracy",
            patience=8,
            restore_best_weights=True,
            verbose=1
        ),
        callbacks.ReduceLROnPlateau(
            monitor="val_loss",
            factor=0.2,
            patience=3,
            min_lr=1e-6,
            verbose=1
        )
    ]

    # 6. Train
    print("\nüî• Training Started...")
    model.fit(
        train_ds,
        epochs=config.EPOCHS,
        initial_epoch=initial_epoch,
        validation_data=val_ds,
        callbacks=callbacks_list,
        verbose=1
    )
    
    print(f"\n‚úÖ Training Complete. Best model saved to: {FINAL_MODEL_PATH}")

if __name__ == "__main__":
    main()
