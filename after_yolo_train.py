"""
Phase 2 Trainer: YOLO-Centric Fine-Tuning.

This script trains the EfficientNet-B5 model on the 'Cleaned' dataset
generated by the YOLO Processor.

UPDATES:
- Aligned strictly with train.py logic.
- Uses MASTER_CLASSES to ensure Output Vector matches Global Model exactly.
- Removed .cache() to prevent RAM OOM.
- Moved augmentation to CPU pipeline.
"""

import os
import numpy as np
import tensorflow as tf
import keras
from keras import callbacks
from pathlib import Path

# Local Imports
from src import config
from src import augmentation
from src import vision_model

# Constants
AUTOTUNE = tf.data.AUTOTUNE

# Configuration Overrides for Local Model
YOLO_DATA_DIR = config.DATA_DIR / "yolo_processed"
TRAIN_DIR = YOLO_DATA_DIR / "train"
VAL_DIR = YOLO_DATA_DIR / "val"
CHECKPOINT_PATH = config.CHECKPOINT_DIR / "model_yolo_best.keras"
FINAL_MODEL_PATH = config.MODELS_DIR / "model_yolo_final.keras"

# Force the exact same class order as the Global Model
# This prevents "Local model has 98 classes, Global has 100" errors
MASTER_CLASSES = np.load(config.LABELS_PATH).tolist()

def load_dataset(directory: Path, is_training: bool = False):
    """
    Loads and preprocesses the image dataset using Streaming (No Cache).
    Uses MASTER_CLASSES to enforce fixed output size.
    """
    if not directory.exists():
        raise FileNotFoundError(f"Dataset not found at {directory}. Run 'src/data_tools/yolo_processor.py' first.")

    raw_dataset = keras.utils.image_dataset_from_directory(
        directory,
        labels="inferred",
        label_mode="categorical",
        class_names=MASTER_CLASSES, # <--- ENFORCED CONSISTENCY
        color_mode="rgb",
        batch_size=config.BATCH_SIZE,
        image_size=config.IMG_SIZE,
        shuffle=True,
        seed=config.SEED,
        interpolation="bilinear"
    )

    class_names = raw_dataset.class_names
    dataset = raw_dataset

    # Apply Augmentation Pipeline (Runs on CPU/Parallel)
    if is_training:
        aug_pipeline = augmentation.get_augmentation_pipeline()
        dataset = dataset.map(
            lambda x, y: (aug_pipeline(x, training=True), y),
            num_parallel_calls=AUTOTUNE
        )

    # Prefetch for GPU speed (But DO NOT cache full dataset)
    dataset = dataset.prefetch(buffer_size=AUTOTUNE)
    
    return dataset, class_names

def main():
    print(f"ðŸš€ Starting YOLO-Centric Training Pipeline")
    print(f"TensorFlow Version: {tf.__version__}")
    print(f"Training on device: {tf.config.list_physical_devices('GPU')}")
    print(f"ðŸ“‚ Data Source: {YOLO_DATA_DIR}")

    # 1. Load Data
    print("\n--- Loading Data ---")
    train_ds, train_class_names = load_dataset(TRAIN_DIR, is_training=True)
    val_ds, val_class_names = load_dataset(VAL_DIR, is_training=False)
    
    # We trust MASTER_CLASSES count, not just what's found in folders
    num_classes = len(MASTER_CLASSES) 
    print(f"â„¹ï¸ Enforcing {num_classes} classes (from class_names.npy).")

    # 2. Build or Resume Model
    initial_epoch = 0
    
    if os.path.exists(CHECKPOINT_PATH):
        print(f"\n--- Resuming from Checkpoint: {CHECKPOINT_PATH} ---")
        try:
            # Attempt to load full model state (Optimizer + Weights)
            custom_objects = {"RandomGaussianBlur": augmentation.RandomGaussianBlur}
            model = keras.models.load_model(CHECKPOINT_PATH, custom_objects=custom_objects)
            print(">> Model and Optimizer State Loaded Successfully.")
        except Exception as e:
            print(f">> Warning: Could not load full state ({e}). Rebuilding and loading weights.")
            model = vision_model.build_model(num_classes=num_classes)
            model.load_weights(CHECKPOINT_PATH)
    else:
        print("\n--- Building New Local Model ---")
        model = vision_model.build_model(num_classes=num_classes)

    # 3. Define Callbacks
    # We add CSVLogger to track the specific local training metrics
    csv_logger = callbacks.CSVLogger(str(config.PROCESSED_DIR / "training_log_yolo.csv"))

    checkpoint_cb = callbacks.ModelCheckpoint(
        filepath=str(CHECKPOINT_PATH),
        save_best_only=True,
        monitor="val_accuracy",
        mode="max",
        verbose=1
    )

    early_stopping_cb = callbacks.EarlyStopping(
        monitor="val_accuracy",
        patience=10,
        restore_best_weights=True,
        verbose=1
    )

    reduce_lr_cb = callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.2,
        patience=3,
        min_lr=1e-6,
        verbose=1
    )

    # 4. Train
    print("\n--- Starting Training Job ---")
    history = model.fit(
        train_ds,
        epochs=50, 
        initial_epoch=initial_epoch,
        validation_data=val_ds,
        callbacks=[checkpoint_cb, early_stopping_cb, reduce_lr_cb, csv_logger],
        verbose=1
    )

    # 5. Save Final Artifact
    print(f"\n--- Saving Final Model to {FINAL_MODEL_PATH} ---")
    model.save(FINAL_MODEL_PATH)
    print("âœ… Local Training Complete.")

if __name__ == "__main__":
    # Ensure directories exist
    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)
    main()